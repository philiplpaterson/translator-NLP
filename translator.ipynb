{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor: Philip Paterson\\n\\nReferenced PyTorch documentation\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import io\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from MyTransformer import MyTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from util.bleu import get_bleu\n",
    "'''\n",
    "Author: Philip Paterson\n",
    "\n",
    "Referenced PyTorch documentation\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given code\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    my_counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as filehandle:\n",
    "        for str in filehandle:\n",
    "            my_counter.update(tokenizer(str))\n",
    "    return Vocab(my_counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "def tokenize_sentence(sentence : str, vocab, tokenizer):\n",
    "    '''\n",
    "    Tokenizes the given sentence\n",
    "    '''\n",
    "    tok_arr = [vocab['<bos>']]\n",
    "    for token in tokenizer(sentence.rstrip(\"\\n\")):\n",
    "        if token in vocab:\n",
    "            tok_arr.append(vocab[token])\n",
    "        else:\n",
    "            tok_arr.append(vocab['<unk>'])\n",
    "    tok_arr.append(vocab['<eos>'])\n",
    "\n",
    "    return tok_arr\n",
    "\n",
    "def tokenize_text(iterator, vocab, tokenizer) -> list:\n",
    "    tokenized_text = []\n",
    "    for sentence in iterator:\n",
    "        tokenized_sentence = torch.Tensor(tokenize_sentence(sentence, vocab, tokenizer))\n",
    "        tokenized_text.append(tokenized_sentence)\n",
    "    return tokenized_text\n",
    "\n",
    "def create_batch(each_data_batch, PAD_IDX):\n",
    "    '''\n",
    "    Creates a batch\n",
    "    '''\n",
    "    de_batch, en_batch = [], []\n",
    "    for (de_item, en_item) in each_data_batch:\n",
    "        de_batch.append(de_item)\n",
    "        en_batch.append(en_item)\n",
    "\n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    return de_batch, en_batch\n",
    "\n",
    "def run_one_epoch(epoch_index : int, model, dataloader : DataLoader, loss_func, optimizer, pad_idx, split : str, device : str = 'cpu'):\n",
    "    running_loss = 0.\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        # Change the device and format\n",
    "        tgt = tgt.type(torch.LongTensor)\n",
    "        src = src.to(device=device)\n",
    "        tgt = tgt.to(device=device)\n",
    "        \n",
    "        # Zero the gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_input = tgt[:-1]\n",
    "        tgt_out = tgt[1:]\n",
    "\n",
    "        # Make the predictions from the forward pass\n",
    "        logits = model(src=src, trg=tgt_input, memory_key_padding_mask = None, PAD_IDX=torch.tensor(pad_idx, device=device))\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_func(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        \n",
    "        # If we're training the model\n",
    "        if split == 'TRAIN':\n",
    "            # Compute loss gradients using backward\n",
    "            loss.backward()\n",
    "\n",
    "            # Make adjustments to the learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # Sum the losses and accuracies\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "def train(\n",
    "        model: nn.Module,\n",
    "        train_dataloader : DataLoader,\n",
    "        valid_dataloader : DataLoader,\n",
    "        test_dataloader : DataLoader,\n",
    "        loss_func, optimizer,\n",
    "        pad_idx,\n",
    "        tgt_vocab,\n",
    "        device = 'cpu'\n",
    "    ):\n",
    "\n",
    "    EPOCHS = 5\n",
    "\n",
    "    # Referenced https://pieriantraining.com/reversing-keys-and-values-in-a-python-dictionary/ \n",
    "    # for reversing a dictionary\n",
    "    reversed_tgt_vocab = {value: key for key, value in tgt_vocab.items()}\n",
    "    \n",
    "    # Initialize parameters\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # Loop variables\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    bleu_scores = []\n",
    "    best_bleu_score = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"---Epoch {epoch + 1}---\")\n",
    "        \n",
    "        # Run the training\n",
    "        model.train(True)\n",
    "        \n",
    "        avg_train_loss = run_one_epoch(\n",
    "            epoch,\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            loss_func,\n",
    "            optimizer,\n",
    "            pad_idx,\n",
    "            'TRAIN',\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        # Evaluate the model off of the validation dataset\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            avg_valid_loss = run_one_epoch(\n",
    "                epoch,\n",
    "                model,\n",
    "                valid_dataloader,\n",
    "                loss_func,\n",
    "                optimizer,\n",
    "                pad_idx,\n",
    "                'VALID',\n",
    "                device\n",
    "            )\n",
    "\n",
    "        # Calculate the bleu score\n",
    "        with torch.no_grad():\n",
    "            bleu_score = calc_bleu_score(\n",
    "                model=model,\n",
    "                dataloader=test_dataloader,\n",
    "                device=device,\n",
    "                tgt_vocab=tgt_vocab,\n",
    "                reversed_tgt_vocab=reversed_tgt_vocab\n",
    "            )\n",
    "\n",
    "        # Save the best model parameters\n",
    "        if bleu_score > best_bleu_score:\n",
    "            best_bleu_score = bleu_score\n",
    "            torch.save(model.state_dict(), 'saved_model.pt')\n",
    "        \n",
    "        # Append the losses and bleu score\n",
    "        avg_train_losses.append(avg_train_loss)\n",
    "        avg_valid_losses.append(avg_valid_loss)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        # Print the per epoch losses and accuracies\n",
    "        print(f\"Loss: | Train: {avg_train_loss:.5f} | Validation: {avg_valid_loss:.5f}\")\n",
    "        print(f\"BLEU Score: {bleu_score}\")\n",
    "    \n",
    "    stats = {\n",
    "        'training' : {\n",
    "            'losses' : avg_train_losses,\n",
    "        },\n",
    "        'validation' : {\n",
    "            'losses' : avg_valid_losses,\n",
    "        },\n",
    "        'testing:' : {\n",
    "            'bleu' : bleu_scores\n",
    "        },\n",
    "        'epochs' : np.arange(0, EPOCHS)\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "def get_data(src_data_path, tgt_data_path, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer):\n",
    "    '''\n",
    "    Generic function to get data. src would be the you're translating from,\n",
    "    and tgt is the language you're translating into.\n",
    "    '''\n",
    "\n",
    "    src_data_raw_iter = iter(io.open(src_data_path, encoding=\"utf8\"))\n",
    "    tgt_data_raw_iter = iter(io.open(tgt_data_path, encoding=\"utf8\"))\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    src_tokenized = tokenize_text(src_data_raw_iter, src_vocab, src_tokenizer)\n",
    "    tgt_tokenized = tokenize_text(tgt_data_raw_iter, tgt_vocab, tgt_tokenizer)\n",
    "\n",
    "    # Group the data together\n",
    "    data = list(zip(src_tokenized, tgt_tokenized))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_dataloader(src_data_path, tgt_data_path, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer, batch_size):\n",
    "    data = get_data(\n",
    "        src_data_path=src_data_path,\n",
    "        tgt_data_path=tgt_data_path,\n",
    "        src_vocab=src_vocab,\n",
    "        tgt_vocab=tgt_vocab,\n",
    "        src_tokenizer=src_tokenizer,\n",
    "        tgt_tokenizer=tgt_tokenizer\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: create_batch(batch, src_vocab['<pad>'])\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def calc_bleu_score(model : nn.Module, dataloader : DataLoader, device, tgt_vocab, reversed_tgt_vocab):\n",
    "    SENTENCE_GENERATION_MAX = 500\n",
    "    \n",
    "    pad_idx = tgt_vocab['<pad>']\n",
    "\n",
    "    running_bleu = 0.\n",
    "    sent_idx = 0 # Keeps track of how many sentences generated\n",
    "    for src, tgt in dataloader:\n",
    "        # Stops the generation of sentences\n",
    "        if sent_idx >= SENTENCE_GENERATION_MAX:\n",
    "            break\n",
    "\n",
    "        tgt = tgt.type(torch.LongTensor)\n",
    "        src = src.to(device=device)\n",
    "        tgt = tgt.to(device=device)\n",
    "\n",
    "        # print(\"SRC SHAPE:\", src.shape) # TODO: Remove\n",
    "\n",
    "        # pred_sent_beginning = [[tgt_vocab['<bos>']]] * src.size(0)\n",
    "        next_tok = tgt_vocab['<bos>']\n",
    "        pred_sent_beginning = [[tgt_vocab['<bos>']]]\n",
    "\n",
    "        pred_sent = torch.tensor(pred_sent_beginning).to(device=device) # Initialize the predicted sentence\n",
    "        # print(pred_sent) # TODO: Remove\n",
    "\n",
    "        # Iteratively test to build the prediction sentence\n",
    "        while len(pred_sent) <= len(tgt) and next_tok != tgt_vocab['<eos>']:\n",
    "            logits = model(src=src, trg=pred_sent, memory_key_padding_mask=None, PAD_IDX=torch.tensor(pad_idx, device=device))\n",
    "            # print(\"logits[-1]\", logits[-1][0]) # TODO: Remove\n",
    "            next_tok = torch.argmax(logits[-1][0], dim=-1)\n",
    "            # print(\"Next token:\", next_tok) # TODO: Remove\n",
    "            pred_sent = torch.cat((pred_sent, next_tok.unsqueeze(0).unsqueeze(0)), dim=0)\n",
    "            # print(f\"Predicted sentence with shape {pred_sent.shape}: {pred_sent}\") # TODO: Remove\n",
    "\n",
    "        # Convert the tokenized predicted and target sentences to list of words\n",
    "        trg_sent_list = detokenize(tgt.squeeze(1).tolist(), reversed_tgt_vocab)\n",
    "        pred_sent_list = detokenize(pred_sent.squeeze(1).tolist(), reversed_tgt_vocab)\n",
    "\n",
    "        # Get the bleu scores\n",
    "        bleu = get_bleu(hypotheses=pred_sent_list, reference=trg_sent_list)\n",
    "\n",
    "        running_bleu += bleu\n",
    "        \n",
    "        sent_idx += 1\n",
    "    \n",
    "    average_total_bleu = running_bleu / len(dataloader) # Calculate the average bleu score\n",
    "\n",
    "    return average_total_bleu\n",
    "\n",
    "def detokenize(tokenized_sentence, reversed_vocab):\n",
    "    '''\n",
    "    Takes a tokenized sentence and converts it to a sentence of words.\n",
    "    '''\n",
    "\n",
    "    # Build the translated sentence\n",
    "    sentence = []\n",
    "    for token in tokenized_sentence:\n",
    "        sentence.append(reversed_vocab[token])\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a main function!!!\n",
    "\n",
    "german_train_path = 'data/train.de'\n",
    "english_train_path = 'data/train.en'\n",
    "german_test_path = 'data/test.de'\n",
    "english_test_path = 'data/test.en'\n",
    "german_valid_path = 'data/val.de'\n",
    "english_valid_path = 'data/val.en'\n",
    "\n",
    "german_vocab_path = 'German_vocab.pth'\n",
    "english_vocab_path = 'English_vocab.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Get the tokenizer\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# Get the vocabulary\n",
    "de_vocab = torch.load(german_vocab_path, de_tokenizer).stoi\n",
    "en_vocab = torch.load(english_vocab_path, en_tokenizer).stoi\n",
    "\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "\n",
    "# Get the tokenized dataloader\n",
    "train_dataloader = get_dataloader(\n",
    "    src_data_path=german_train_path,\n",
    "    tgt_data_path=english_train_path,\n",
    "    src_vocab=de_vocab,\n",
    "    tgt_vocab=en_vocab,\n",
    "    src_tokenizer=de_tokenizer,\n",
    "    tgt_tokenizer=en_tokenizer,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Get the validation dataloader\n",
    "valid_dataloader = get_dataloader(\n",
    "    src_data_path=german_valid_path,\n",
    "    tgt_data_path=english_valid_path,\n",
    "    src_vocab=de_vocab,\n",
    "    tgt_vocab=en_vocab,\n",
    "    src_tokenizer=de_tokenizer,\n",
    "    tgt_tokenizer=en_tokenizer,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Get the test datalaoder\n",
    "test_data = get_data(\n",
    "    src_data_path=german_test_path,\n",
    "    tgt_data_path=english_test_path,\n",
    "    src_vocab=de_vocab,\n",
    "    tgt_vocab=en_vocab,\n",
    "    src_tokenizer=de_tokenizer,\n",
    "    tgt_tokenizer=en_tokenizer,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: create_batch(batch, de_vocab['<pad>'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philip/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Epoch 1---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philip/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: | Train: 4.29864 | Validation: 3.28098\n",
      "BLEU Score: 29.82609273081657\n",
      "---Epoch 2---\n",
      "Loss: | Train: 3.03652 | Validation: 2.66561\n",
      "BLEU Score: 34.87597976220966\n",
      "---Epoch 3---\n",
      "Loss: | Train: 2.53995 | Validation: 2.33234\n",
      "BLEU Score: 37.107249226847934\n",
      "---Epoch 4---\n",
      "Loss: | Train: 2.21552 | Validation: 2.15543\n",
      "BLEU Score: 36.9990221843552\n",
      "---Epoch 5---\n",
      "Loss: | Train: 1.98241 | Validation: 2.02075\n",
      "BLEU Score: 38.186847022697584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'training': {'losses': [4.298642394823959,\n",
       "   3.036516889059031,\n",
       "   2.5399531505862303,\n",
       "   2.2155243727443237,\n",
       "   1.9824136542905915]},\n",
       " 'validation': {'losses': [3.280981183052063,\n",
       "   2.665611170232296,\n",
       "   2.3323374316096306,\n",
       "   2.1554328873753548,\n",
       "   2.0207463167607784]},\n",
       " 'testing:': {'bleu': [29.82609273081657,\n",
       "   34.87597976220966,\n",
       "   37.107249226847934,\n",
       "   36.9990221843552,\n",
       "   38.186847022697584]},\n",
       " 'epochs': array([0, 1, 2, 3, 4])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "'''\n",
    "1. No. of encoder layers: 3.\n",
    "2. No. of decoder layers: 3.\n",
    "3. Embedding dimension: 512.\n",
    "4. Feedforward dimension: 512.\n",
    "5. No. of Attention head: 8.\n",
    "'''\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "\n",
    "model = MyTransformer(\n",
    "    num_encoder_layers=NUM_DECODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    emb_size=EMB_SIZE,\n",
    "    nhead=NHEAD,\n",
    "    src_vocab_size=len(de_vocab),\n",
    "    tgt_vocab_size=len(en_vocab),\n",
    "    dim_feedforward=FFN_HID_DIM\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "stats = train(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    loss_func=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    pad_idx=PAD_IDX,\n",
    "    tgt_vocab=de_vocab,\n",
    "    device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
