{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor: Philip Paterson\\n\\nReferenced PyTorch documentation\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import io\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from MyTransformer import MyTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from util.bleu import get_bleu\n",
    "'''\n",
    "Author: Philip Paterson\n",
    "\n",
    "Referenced PyTorch documentation\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given code\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    my_counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as filehandle:\n",
    "        for str in filehandle:\n",
    "            my_counter.update(tokenizer(str))\n",
    "    return Vocab(my_counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "def tokenize_sentence(sentence : str, vocab, tokenizer):\n",
    "    '''\n",
    "    Tokenizes the given sentence\n",
    "    '''\n",
    "    tok_arr = [vocab['<bos>']]\n",
    "    for token in tokenizer(sentence.rstrip(\"\\n\")):\n",
    "        if token in vocab:\n",
    "            tok_arr.append(vocab[token])\n",
    "        else:\n",
    "            tok_arr.append(vocab['<unk>'])\n",
    "    tok_arr.append(vocab['<eos>'])\n",
    "\n",
    "    return tok_arr\n",
    "\n",
    "def tokenize_text(iterator, vocab, tokenizer) -> list:\n",
    "    tokenized_text = []\n",
    "    for sentence in iterator:\n",
    "        tokenized_sentence = torch.Tensor(tokenize_sentence(sentence, vocab, tokenizer))\n",
    "        tokenized_text.append(tokenized_sentence)\n",
    "    return tokenized_text\n",
    "\n",
    "def create_batch(each_data_batch, PAD_IDX):\n",
    "    '''\n",
    "    Creates a batch\n",
    "    '''\n",
    "    de_batch, en_batch = [], []\n",
    "    for (de_item, en_item) in each_data_batch:\n",
    "        de_batch.append(de_item)\n",
    "        en_batch.append(en_item)\n",
    "\n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    return de_batch, en_batch\n",
    "\n",
    "def run_one_epoch(epoch_index : int, model, dataloader : DataLoader, loss_func, optimizer, pad_idx, split : str, device : str = 'cpu'):\n",
    "    running_loss = 0.\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        # Change the device and format\n",
    "        tgt = tgt.type(torch.LongTensor)\n",
    "        src = src.to(device=device)\n",
    "        tgt = tgt.to(device=device)\n",
    "        \n",
    "        # Zero the gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_input = tgt[:-1]\n",
    "        tgt_out = tgt[1:]\n",
    "\n",
    "        # Make the predictions from the forward pass\n",
    "        logits = model(src=src, trg=tgt_input, memory_key_padding_mask = None, PAD_IDX=torch.tensor(pad_idx, device=device))\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_func(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        \n",
    "        # If we're training the model\n",
    "        if split == 'TRAIN':\n",
    "            # Compute loss gradients using backward\n",
    "            loss.backward()\n",
    "\n",
    "            # Make adjustments to the learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # Sum the losses and accuracies\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "def train(\n",
    "        model: nn.Module,\n",
    "        train_dataloader : DataLoader,\n",
    "        valid_dataloader : DataLoader,\n",
    "        test_dataloader : DataLoader,\n",
    "        loss_func, optimizer,\n",
    "        pad_idx,\n",
    "        tgt_vocab,\n",
    "        device = 'cpu'\n",
    "    ):\n",
    "\n",
    "    EPOCHS = 5\n",
    "\n",
    "    # Referenced https://pieriantraining.com/reversing-keys-and-values-in-a-python-dictionary/ \n",
    "    # for reversing a dictionary\n",
    "    reversed_tgt_vocab = {value: key for key, value in tgt_vocab.items()}\n",
    "    \n",
    "    # Initialize parameters\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # Loop variables\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    bleu_scores = []\n",
    "    best_bleu_score = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"---Epoch {epoch + 1}---\")\n",
    "        \n",
    "        # Run the training\n",
    "        model.train(True)\n",
    "        \n",
    "        avg_train_loss = run_one_epoch(\n",
    "            epoch,\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            loss_func,\n",
    "            optimizer,\n",
    "            pad_idx,\n",
    "            'TRAIN',\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        # Evaluate the model off of the validation dataset\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            avg_valid_loss = run_one_epoch(\n",
    "                epoch,\n",
    "                model,\n",
    "                valid_dataloader,\n",
    "                loss_func,\n",
    "                optimizer,\n",
    "                pad_idx,\n",
    "                'VALID',\n",
    "                device\n",
    "            )\n",
    "\n",
    "        # Calculate the bleu score\n",
    "        with torch.no_grad():\n",
    "            bleu_score = calc_bleu_score(\n",
    "                model=model,\n",
    "                dataloader=test_dataloader,\n",
    "                device=device,\n",
    "                tgt_vocab=tgt_vocab,\n",
    "                reversed_tgt_vocab=reversed_tgt_vocab\n",
    "            )\n",
    "\n",
    "        # Save the best model parameters\n",
    "        if bleu_score > best_bleu_score:\n",
    "            best_bleu_score = bleu_score\n",
    "            torch.save(model.state_dict(), 'saved_model.pt')\n",
    "        \n",
    "        # Append the losses and bleu score\n",
    "        avg_train_losses.append(avg_train_loss)\n",
    "        avg_valid_losses.append(avg_valid_loss)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        # Print the per epoch losses and accuracies\n",
    "        print(f\"Loss: | Train: {avg_train_loss:.5f} | Validation: {avg_valid_loss:.5f}\")\n",
    "        print(f\"BLEU Score: {bleu_score}\")\n",
    "    \n",
    "    stats = {\n",
    "        'training' : {\n",
    "            'losses' : avg_train_losses,\n",
    "        },\n",
    "        'validation' : {\n",
    "            'losses' : avg_valid_losses,\n",
    "        },\n",
    "        'testing:' : {\n",
    "            'bleu' : bleu_scores\n",
    "        },\n",
    "        'epochs' : np.arange(0, EPOCHS)\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "def test_translator(src_data_path : str, tgt_data_path : str, src_vocab_path : str, tgt_vocab_path : str, model_param_path : str):\n",
    "    '''\n",
    "    TEST FUNCTION For testing the translator model.\n",
    "\n",
    "    Only works for testing translation from German to English.\n",
    "    '''\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    \n",
    "    SRC_TOKENIZER_LANGUAGE = 'de_core_news_sm'\n",
    "    TGT_TOKENIZER_LANGUAGE = 'en_core_web_sm'\n",
    "\n",
    "    # Get the tokenizer\n",
    "    src_tokenizer = get_tokenizer('spacy', language=SRC_TOKENIZER_LANGUAGE)\n",
    "    tgt_tokenizer = get_tokenizer('spacy', language=TGT_TOKENIZER_LANGUAGE)\n",
    "\n",
    "    # Get the vocabulary\n",
    "    src_vocab = torch.load(src_vocab_path, src_tokenizer).stoi\n",
    "    tgt_vocab = torch.load(tgt_vocab_path, tgt_tokenizer).stoi\n",
    "    \n",
    "    # Get the data\n",
    "    data = get_data(\n",
    "        src_data_path=src_data_path,\n",
    "        tgt_data_path=tgt_data_path,\n",
    "        src_vocab=src_vocab,\n",
    "        tgt_vocab=tgt_vocab,\n",
    "        src_tokenizer=src_tokenizer,\n",
    "        tgt_tokenizer=tgt_tokenizer,\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        data,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: create_batch(batch, src_vocab['<pad>'])\n",
    "    )\n",
    "\n",
    "    # Define and create the model\n",
    "    NUM_ENCODER_LAYERS = 3\n",
    "    NUM_DECODER_LAYERS = 3\n",
    "    EMB_SIZE = 512\n",
    "    NHEAD = 8\n",
    "    FFN_HID_DIM = 512\n",
    "\n",
    "    model = MyTransformer(\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "        emb_size=EMB_SIZE,\n",
    "        nhead=NHEAD,\n",
    "        src_vocab_size=len(src_vocab),\n",
    "        tgt_vocab_size=len(tgt_vocab),\n",
    "        dim_feedforward=FFN_HID_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_param_path, map_location=device))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Create a reversed dictionary for calculating the bleu score\n",
    "    reversed_tgt_vocab = {value: key for key, value in tgt_vocab.items()}\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        bleu_score = calc_bleu_score(\n",
    "                model=model,\n",
    "                dataloader=dataloader,\n",
    "                device=device,\n",
    "                tgt_vocab=tgt_vocab,\n",
    "                reversed_tgt_vocab=reversed_tgt_vocab\n",
    "            )\n",
    "        \n",
    "    print(f\"Testing BLEU Score: {bleu_score}\")\n",
    "    return bleu_score\n",
    "    \n",
    "\n",
    "def get_data(src_data_path, tgt_data_path, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer):\n",
    "    '''\n",
    "    Generic function to get data. src would be the you're translating from,\n",
    "    and tgt is the language you're translating into.\n",
    "    '''\n",
    "\n",
    "    src_data_raw_iter = iter(io.open(src_data_path, encoding=\"utf8\"))\n",
    "    tgt_data_raw_iter = iter(io.open(tgt_data_path, encoding=\"utf8\"))\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    src_tokenized = tokenize_text(src_data_raw_iter, src_vocab, src_tokenizer)\n",
    "    tgt_tokenized = tokenize_text(tgt_data_raw_iter, tgt_vocab, tgt_tokenizer)\n",
    "\n",
    "    # Group the data together\n",
    "    data = list(zip(src_tokenized, tgt_tokenized))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_dataloader(src_data_path, tgt_data_path, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer, batch_size):\n",
    "    data = get_data(\n",
    "        src_data_path=src_data_path,\n",
    "        tgt_data_path=tgt_data_path,\n",
    "        src_vocab=src_vocab,\n",
    "        tgt_vocab=tgt_vocab,\n",
    "        src_tokenizer=src_tokenizer,\n",
    "        tgt_tokenizer=tgt_tokenizer\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: create_batch(batch, src_vocab['<pad>'])\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def calc_bleu_score(model : nn.Module, dataloader : DataLoader, device, tgt_vocab, reversed_tgt_vocab):\n",
    "    SENTENCE_GENERATION_MAX = 500\n",
    "    \n",
    "    pad_idx = tgt_vocab['<pad>']\n",
    "\n",
    "    running_bleu = 0.\n",
    "    sent_idx = 0 # Keeps track of how many sentences generated\n",
    "    for src, tgt in dataloader:\n",
    "        # Stops the generation of sentences\n",
    "        if sent_idx >= SENTENCE_GENERATION_MAX:\n",
    "            break\n",
    "\n",
    "        tgt = tgt.type(torch.LongTensor)\n",
    "        src = src.to(device=device)\n",
    "        tgt = tgt.to(device=device)\n",
    "\n",
    "        # print(\"SRC SHAPE:\", src.shape) # TODO: Remove\n",
    "\n",
    "        # pred_sent_beginning = [[tgt_vocab['<bos>']]] * src.size(0)\n",
    "        next_tok = tgt_vocab['<bos>']\n",
    "        pred_sent_beginning = [[tgt_vocab['<bos>']]]\n",
    "\n",
    "        pred_sent = torch.tensor(pred_sent_beginning).to(device=device) # Initialize the predicted sentence\n",
    "\n",
    "        # Iteratively test to build the prediction sentence\n",
    "        while len(pred_sent) <= len(tgt) and next_tok != tgt_vocab['<eos>']:\n",
    "            logits = model(src=src, trg=pred_sent, memory_key_padding_mask=None, PAD_IDX=torch.tensor(pad_idx, device=device))\n",
    "            next_tok = torch.argmax(logits[-1][0], dim=-1)\n",
    "            pred_sent = torch.cat((pred_sent, next_tok.unsqueeze(0).unsqueeze(0)), dim=0)\n",
    "\n",
    "        # Convert the tokenized predicted and target sentences to list of words\n",
    "        trg_sent_list = detokenize(tgt.squeeze(1).tolist(), reversed_tgt_vocab)\n",
    "        pred_sent_list = detokenize(pred_sent.squeeze(1).tolist(), reversed_tgt_vocab)\n",
    "\n",
    "        # Get the bleu scores\n",
    "        bleu = get_bleu(hypotheses=pred_sent_list, reference=trg_sent_list)\n",
    "\n",
    "        running_bleu += bleu\n",
    "        \n",
    "        sent_idx += 1\n",
    "    \n",
    "    average_total_bleu = running_bleu / len(dataloader) # Calculate the average bleu score\n",
    "\n",
    "    return average_total_bleu\n",
    "\n",
    "def detokenize(tokenized_sentence, reversed_vocab):\n",
    "    '''\n",
    "    Takes a tokenized sentence and converts it to a sentence of words.\n",
    "    '''\n",
    "\n",
    "    # Build the translated sentence\n",
    "    sentence = []\n",
    "    for token in tokenized_sentence:\n",
    "        sentence.append(reversed_vocab[token])\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a main function!!!\n",
    "\n",
    "german_train_path = 'data/train.de'\n",
    "english_train_path = 'data/train.en'\n",
    "german_test_path = 'data/test.de'\n",
    "english_test_path = 'data/test.en'\n",
    "german_valid_path = 'data/val.de'\n",
    "english_valid_path = 'data/val.en'\n",
    "\n",
    "german_vocab_path = 'German_vocab.pth'\n",
    "english_vocab_path = 'English_vocab.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Get the tokenizer\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# Get the vocabulary\n",
    "de_vocab = torch.load(german_vocab_path, de_tokenizer).stoi\n",
    "en_vocab = torch.load(english_vocab_path, en_tokenizer).stoi\n",
    "\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "\n",
    "# Get the tokenized dataloader\n",
    "train_dataloader = get_dataloader(\n",
    "    src_data_path=german_train_path,\n",
    "    tgt_data_path=english_train_path,\n",
    "    src_vocab=de_vocab,\n",
    "    tgt_vocab=en_vocab,\n",
    "    src_tokenizer=de_tokenizer,\n",
    "    tgt_tokenizer=en_tokenizer,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Get the validation dataloader\n",
    "valid_dataloader = get_dataloader(\n",
    "    src_data_path=german_valid_path,\n",
    "    tgt_data_path=english_valid_path,\n",
    "    src_vocab=de_vocab,\n",
    "    tgt_vocab=en_vocab,\n",
    "    src_tokenizer=de_tokenizer,\n",
    "    tgt_tokenizer=en_tokenizer,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Get the test datalaoder\n",
    "test_data = get_data(\n",
    "    src_data_path=german_test_path,\n",
    "    tgt_data_path=english_test_path,\n",
    "    src_vocab=de_vocab,\n",
    "    tgt_vocab=en_vocab,\n",
    "    src_tokenizer=de_tokenizer,\n",
    "    tgt_tokenizer=en_tokenizer,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: create_batch(batch, de_vocab['<pad>'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philip/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Epoch 1---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philip/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mPAD_IDX)\n\u001b[1;32m     26\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.98\u001b[39m), eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m stats \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     30\u001b[0m     train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m     31\u001b[0m     valid_dataloader\u001b[38;5;241m=\u001b[39mvalid_dataloader,\n\u001b[1;32m     32\u001b[0m     test_dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m     33\u001b[0m     loss_func\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m     34\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     35\u001b[0m     pad_idx\u001b[38;5;241m=\u001b[39mPAD_IDX,\n\u001b[1;32m     36\u001b[0m     tgt_vocab\u001b[38;5;241m=\u001b[39mde_vocab,\n\u001b[1;32m     37\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     38\u001b[0m )\n",
      "Cell \u001b[0;32mIn[4], line 113\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, valid_dataloader, test_dataloader, loss_func, optimizer, pad_idx, tgt_vocab, device)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[1;32m    111\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 113\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m run_one_epoch(\n\u001b[1;32m    114\u001b[0m     epoch,\n\u001b[1;32m    115\u001b[0m     model,\n\u001b[1;32m    116\u001b[0m     train_dataloader,\n\u001b[1;32m    117\u001b[0m     loss_func,\n\u001b[1;32m    118\u001b[0m     optimizer,\n\u001b[1;32m    119\u001b[0m     pad_idx,\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAIN\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    121\u001b[0m     device\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Evaluate the model off of the validation dataset\u001b[39;00m\n\u001b[1;32m    125\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m, in \u001b[0;36mrun_one_epoch\u001b[0;34m(epoch_index, model, dataloader, loss_func, optimizer, pad_idx, split, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Sum the losses and accuracies\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     75\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "'''\n",
    "1. No. of encoder layers: 3.\n",
    "2. No. of decoder layers: 3.\n",
    "3. Embedding dimension: 512.\n",
    "4. Feedforward dimension: 512.\n",
    "5. No. of Attention head: 8.\n",
    "'''\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "\n",
    "model = MyTransformer(\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    emb_size=EMB_SIZE,\n",
    "    nhead=NHEAD,\n",
    "    src_vocab_size=len(de_vocab),\n",
    "    tgt_vocab_size=len(en_vocab),\n",
    "    dim_feedforward=FFN_HID_DIM\n",
    ").to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Train the model\n",
    "stats = train(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    loss_func=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    pad_idx=PAD_IDX,\n",
    "    tgt_vocab=de_vocab,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philip/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing BLEU Score: 38.08720844546422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38.08720844546422"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the saved model parameters\n",
    "test_translator(\n",
    "    src_data_path=german_test_path,\n",
    "    tgt_data_path=english_test_path,\n",
    "    src_vocab_path=german_vocab_path,\n",
    "    tgt_vocab_path=english_vocab_path,\n",
    "    model_param_path='saved_model.pt'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
