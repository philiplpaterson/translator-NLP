{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import io\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from MyTransformer import MyTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given code\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    my_counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as filehandle:\n",
    "        for str in filehandle:\n",
    "            my_counter.update(tokenizer(str))\n",
    "    return Vocab(my_counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "def tokenize_sentence(sentence : str, vocab, tokenizer):\n",
    "    '''\n",
    "    Tokenizes the given sentence\n",
    "    '''\n",
    "    tok_arr = [vocab['<bos>']]\n",
    "    for token in tokenizer(sentence.rstrip(\"\\n\")):\n",
    "        if token in vocab:\n",
    "            tok_arr.append(vocab[token])\n",
    "        else:\n",
    "            tok_arr.append(vocab['<unk>'])\n",
    "    tok_arr.append(vocab['<eos>'])\n",
    "\n",
    "    return tok_arr\n",
    "\n",
    "def tokenize_text(iterator, vocab, tokenizer) -> list:\n",
    "    tokenized_text = []\n",
    "    for sentence in iterator:\n",
    "        tokenized_sentence = torch.Tensor(tokenize_sentence(sentence, vocab, tokenizer))\n",
    "        tokenized_text.append(tokenized_sentence)\n",
    "    return tokenized_text\n",
    "\n",
    "def create_batch(each_data_batch, PAD_IDX):\n",
    "    '''\n",
    "    Creates a batch\n",
    "    '''\n",
    "    de_batch, en_batch = [], []\n",
    "    for (de_item, en_item) in each_data_batch:\n",
    "        de_batch.append(de_item)\n",
    "        en_batch.append(en_item)\n",
    "\n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    return de_batch, en_batch\n",
    "\n",
    "def run_one_epoch(epoch_index : int, model, dataloader : DataLoader, loss_func, optimizer, pad_idx, split : str, device : str = 'cpu'):\n",
    "    running_loss = 0.\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        # Change the device and format\n",
    "        tgt = tgt.type(torch.LongTensor)\n",
    "        src = src.to(device=device)\n",
    "        tgt = tgt.to(device=device)\n",
    "        \n",
    "        # Zero the gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_input = tgt[:-1]\n",
    "        tgt_out = tgt[1:]\n",
    "\n",
    "        # Make the predictions from the forward pass\n",
    "        logits = model(src=src, trg=tgt_input, memory_key_padding_mask = None, PAD_IDX=torch.tensor(pad_idx, device=device))\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_func(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        \n",
    "        # If we're training the model\n",
    "        if split == 'TRAIN':\n",
    "            # Compute loss gradients using backward\n",
    "            loss.backward()\n",
    "\n",
    "            # Make adjustments to the learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # Sum the losses and accuracies\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "def train(model: nn.Module, train_dataloader : DataLoader, valid_dataloader : DataLoader, loss_func, optimizer, pad_idx, device = 'cpu'):\n",
    "    EPOCHS = 5\n",
    "\n",
    "    avg_train_losses = []\n",
    "\n",
    "    avg_valid_losses = []\n",
    "\n",
    "    best_valid_accuracy = 0\n",
    "    \n",
    "    # Initialize parameters\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"---Epoch {epoch + 1}---\")\n",
    "        # Ensure gradient tracking is on\n",
    "        model.train(True)\n",
    "        \n",
    "        avg_train_loss = run_one_epoch(\n",
    "            epoch,\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            loss_func,\n",
    "            optimizer,\n",
    "            pad_idx,\n",
    "            'TRAIN',\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        # Evaluate the model off of the validation dataset\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            avg_valid_loss = run_one_epoch(\n",
    "                epoch,\n",
    "                model,\n",
    "                valid_dataloader,\n",
    "                loss_func,\n",
    "                optimizer,\n",
    "                pad_idx,\n",
    "                'VALID',\n",
    "                device\n",
    "            )\n",
    "\n",
    "        # TODO: IMPLEMENT LATER\n",
    "        # # Save the best model parameters\n",
    "        # if avg_valid_accuracy > best_valid_accuracy:\n",
    "        #     best_valid_accuracy = avg_valid_accuracy\n",
    "        #     torch.save(model.state_dict(), 'saved_model.pt')\n",
    "        \n",
    "        # Print the per epoch losses and accuracies\n",
    "        print(f\"Loss:     | Train: {avg_train_loss:.5f}     | Validation: {avg_valid_loss:.5f}\")\n",
    "    \n",
    "    stats = {\n",
    "        'training' : {\n",
    "            'losses' : avg_train_losses,\n",
    "        },\n",
    "        'validation' : {\n",
    "            'losses' : avg_valid_losses,\n",
    "        },\n",
    "        'epochs' : np.arange(0, EPOCHS)\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "def calc_acc(prediction, actual):\n",
    "    # Calculate accuracy\n",
    "    accuracy = torch.sum(torch.argmax(prediction, 1) == torch.argmax(actual, 1))/actual.shape[0]\n",
    "    return accuracy.item()\n",
    "\n",
    "def get_data(src_data_path, trg_data_path, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer):\n",
    "    '''\n",
    "    Generic function to get data. src would be the you're translating from,\n",
    "    and trg is the language you're translating into.\n",
    "    '''\n",
    "\n",
    "    src_data_raw_iter = iter(io.open(src_data_path, encoding=\"utf8\"))\n",
    "    trg_data_raw_iter = iter(io.open(trg_data_path, encoding=\"utf8\"))\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    src_tokenized = tokenize_text(src_data_raw_iter, src_vocab, src_tokenizer)\n",
    "    trg_tokenized = tokenize_text(trg_data_raw_iter, trg_vocab, trg_tokenizer)\n",
    "\n",
    "    # Group the data together\n",
    "    data = list(zip(src_tokenized, trg_tokenized))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_dataloader(src_data_path, trg_data_path, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer, batch_size):\n",
    "    data = get_data(\n",
    "        src_data_path=src_data_path,\n",
    "        trg_data_path=trg_data_path,\n",
    "        src_vocab=src_vocab,\n",
    "        trg_vocab=trg_vocab,\n",
    "        src_tokenizer=src_tokenizer,\n",
    "        trg_tokenizer=trg_tokenizer\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: create_batch(batch, src_vocab['<pad>'])\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_train_path = 'data/train.de'\n",
    "english_train_path = 'data/train.en'\n",
    "german_test_path = 'data/test.de'\n",
    "english_test_path = 'data/test.en'\n",
    "german_valid_path = 'data/val.de'\n",
    "english_valid_path = 'data/val.en'\n",
    "\n",
    "german_vocab_path = 'German_vocab.pth'\n",
    "english_vocab_path = 'English_vocab.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Get the tokenizer\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# Get the vocabulary\n",
    "de_vocab = torch.load(german_vocab_path, de_tokenizer).stoi\n",
    "en_vocab = torch.load(english_vocab_path, en_tokenizer).stoi\n",
    "\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "\n",
    "# Get the tokenized dataloader\n",
    "train_dataloader = get_dataloader(\n",
    "    src_data_path=german_train_path,\n",
    "    trg_data_path=english_train_path,\n",
    "    src_vocab=de_vocab,\n",
    "    trg_vocab=en_vocab,\n",
    "    src_tokenizer=de_tokenizer,\n",
    "    trg_tokenizer=en_tokenizer,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Get the validation dataloader\n",
    "valid_dataloader = get_dataloader(\n",
    "    src_data_path=german_valid_path,\n",
    "    trg_data_path=english_valid_path,\n",
    "    src_vocab=de_vocab,\n",
    "    trg_vocab=en_vocab,\n",
    "    src_tokenizer=de_tokenizer,\n",
    "    trg_tokenizer=en_tokenizer,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philip/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Epoch 1---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philip/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:     | Train: 4.27580     | Validation: 3.27088\n",
      "---Epoch 2---\n",
      "Loss:     | Train: 3.03120     | Validation: 2.65624\n",
      "---Epoch 3---\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "'''\n",
    "1. No. of encoder layers: 3.\n",
    "2. No. of decoder layers: 3.\n",
    "3. Embedding dimension: 512.\n",
    "4. Feedforward dimension: 512.\n",
    "5. No. of Attention head: 8.\n",
    "'''\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "\n",
    "model = MyTransformer(\n",
    "    num_encoder_layers=NUM_DECODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    emb_size=EMB_SIZE,\n",
    "    nhead=NHEAD,\n",
    "    src_vocab_size=len(de_vocab),\n",
    "    tgt_vocab_size=len(en_vocab),\n",
    "    dim_feedforward=FFN_HID_DIM\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader,\n",
    "    loss_func=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    pad_idx=PAD_IDX,\n",
    "    device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
